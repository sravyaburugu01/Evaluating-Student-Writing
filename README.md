# Evaluating-Student-Writing

## Goal
The Aim of this project is to identify elements in student writing i.e. we segment text and classify ar- gumentative and rhetorical elements i.e. predict human annotations in essays written by 6th-12th grade students.The projects will encompass a combination of Deep learning and NLP based models. The scope of the project is to make it easier for students to receive feedback on their writing and increase opportunities to improve writing outcomes. Virtual writing tutors and automated writing systems can leverage this project, and teachers may use them to reduce grading time. This project will allow any educational organization to better help young writers develop.

## Related Models and Studies

Attention Is All You Need: This explains how the transformers process an input sequence of words all at once, and they map relevant dependencies between words regardless of how far apart the words appear in the text. Thus, Transformers are highly parallelizable, and we can train large models at a faster rate, and use contextual clues to solve a lot of ambiguity issues that plague text.

BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding: BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.

Language Models are Few-Shot Learners: These models can perform various NLP tasks like question answering, textual entailment, text summarization etc. without any supervised training. These language models need very few to no examples to understand the tasks and perform equivalent or even better than the state-of-the-art models trained in supervised fashion. The paper further explains how large language models develop pattern recognition and other skills using the text data they are trained on. Language models can recognize patterns in data which help them minimize the loss for language modeling task. When presented with few examples and/or a description of what it needs to do, the language models matches the pattern of the examples with what it had learnt in past for similar data and uses that knowledge to perform the tasks. This is a powerful capability of large language models which increases with the increase in the number of parameters of the model.

Attention-based Neural Text Segmentation: This presents a supervised neural approach for text segmentation. Specifically, it talks about attention-based bidirectional LSTM model where sentence embeddings are learned using CNNs and the segments are predicted based on contextual information. Also, this model can automatically handle variable sized context information.

RoBERTa, A Robustly Optimized BERT Pretraining Approach: It is a replication study of BERT pretraining that carefully measures the impact of many key hyperparameters and training data size. The issue with BERT is that, it is significantly undertrained. RoBERT for pretraining NLP systems is an improvement on BERT. Also RoBERT performs optimized training and takes lesser time during pre-training. RoBERT is built on language masking strategy used in BERT, wherein the system learns to predict intentionally hidden sections of text within which otherwise are unannotated language examples. It has modified key hyperparameters, and removes the next-sentence pretraining objective, and performs training with much larger mini-batches and learning rates.

Transformer-based models cannot process long sequences due to their self-attention opera- tion, which scales quadratically with the sequence length. Longformer: The Long-Document Transformer [7] with an attention address this issue. The attention mechanism used in LongFormer scales linearly with sequence length. The long document transformer, accepts wider inputs unto 4096 tokens. Longformerâ€™s attention mechanism uses a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. It employs self attention on both local and global context. LongFormer outperforms RoBERTa.

